%% Copyright (C) 2021 Alessandro Clerici Lorenzini
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Alessandro Clerici Lorenzini
%
% This work consists of the files listed in work.txt


\section{Statistica inferenziale}
La statistica inferenziale tenta di applicare un'inferenza attraverso l'induzione. La statistica inferenziale riprende concetti della statistica descrittiva ma li reinterpreta in senso probabilistico applicando l'induzione.

\subsection{Definizioni}
Gli attori fondamentali della statistica inferenziale sono la popolazione, il campione la statistica o stimatore\footnote{in alcuni contesti, i termini statistica e stimatore differiscono leggermente di significato. Tuttavia in questo testo verranno usati equivalentemente.}, e la stima.

\begin{itemize}
	\item La popolazione è descritta come una variabile aleatoria $X$. La branca della statistica inferenziale in cui la distribuzione di $X$ è nota a meno di uno o più parametri $\theta$ si dice parametrica: $X\sim F(\theta)$. Se anche il modello della distribuzione è ignoto, si tratta di statistica inferenziale non parametrica. Nel resto di questo testo si studierà la statistica inferenziale parametrica;
	\item Poiché non sempre si vuole ricavare o approssimare il parametro di distribuzione ignoto $\theta$, ma talvolta un valore che ne deriva, si introduce la funzione $\tau(\theta)$ a indicare tale valore;
	\item L'informazione conosciuta sulla popolazione è data da un campione, talvolta espresso come una serie di variabili aleatorie $X_1,\dots,X_n$ indipendenti e identicamente distribuite, talvolta come una tupla di loro specificazioni $x_1,\dots,x_n$;
	\item L'operazione di stimare $\tau(\theta)$ consiste nella statistica o stimatore, che è una funzione $t$ che associa a una tupla possibili specificazioni del campione a un numero reale che stima il valore cercato: $t:D_X^n\to\R$. Essendo la statistica una variabile aleatoria, la si rappresenta talvolta (quando si vuole evidenziare tale lettura) con $T$;
	\item Una stima $\hat\tau$ è un'immagine della statistica e approssima $\tau(\theta)$: $\hat\tau = t(x_1,\dots,x_n)$ con istanze $X_1=x_1,\dots,X_n=x_n$.
\end{itemize}

\subsubsection{Stimatori non deviati}
\begin{defin}[stimatore non deviato]
	Uno stimatore $t$ è non deviato per una quantità $\tau(\theta)$ quando il suo valore atteso è uguale a $\tau(\theta)$:
	\begin{equation*}
		\ev{t(X_1,\dots,X_n)} = \tau(\theta)
	\end{equation*}
\end{defin}

\begin{examp}
	Uno stimatore non deviato per stimare il valore atteso, indipendentemente dalla distribuzione, è quello che fa corrispondere alle specificazioni date la loro media:
	\begin{equation*}
		t(x_1,\dots,x_n) = \frac{1}{n} \sum_{i=1}^n x_i \qquad \tau(\theta) = \ev{X}
	\end{equation*}
	Infatti:
	\begin{align*}
		\ev{t} & = \ev{\frac{1}{n} \sum_{i=1}^n X_i} \\
		       & = \frac{1}{n}\sum_{i=1}^n \ev{X_i}  \\
		       & = \frac{1}{n} \cdot n\ev{X_i}       \\
		       & = \ev{X}
	\end{align*}
	Inoltre, calcolando la varianza dello stimatore ci si accorge che essa tende a 0 per campioni molto grandi:
	\begin{equation*}
		\var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n \var(X_i) = \frac{n}{n^2}\var(X) = \frac{\var(X)}{n}
	\end{equation*}
\end{examp}


\subsection{Errore quadratico medio}
Il errore quadratico medio (Mean Square Error o MSE) è un modo di valutare uno stimatore di una quantità ignota:
\begin{defin}[errore quadratico medio]
	\begin{equation*}
		\MSE_{\tau(\theta)}(T) = \ev{(T-\tau(\theta))^2}
	\end{equation*}
	con $T=t(X_1,\dots,X_n)$.
\end{defin}

\begin{defin}
	Il bias di uno stimatore $T$ su $\tau(\theta)$ è la differenza tra il suo valore atteso e $\tau(\theta)$:
	\begin{equation*}
		b_{T(\theta)}(T) = \ev{T} - \tau(\theta)
	\end{equation*}
\end{defin}

In virtù delle definizioni di MSE e bias si può trarre un interessante risultato:
\begin{align*}
	\MSE_{\tau(\theta)}(T) & = \ev{(T-\tau(\theta))^2}                                                                            \\
	                       & = \ev{(T-\ev{T}+\ev{T}-\tau(\theta))^2}                                                              \\
	                       & = \ev{(T-\ev{T})^2+2(T-\ev{T})(\ev{T}-\tau(\theta))+(\ev{T}-\tau(\theta))^2}                         \\
	                       & = \ev{(T-\ev{T})^2} + 2(\ev{T}-\tau(\theta))\underbrace{\ev{T-\ev{T}}}_{0} + (\ev{T}-\tau(\theta))^2 \\
	                       & = \var(T) + (\ev{T}-\tau(\theta))^2                                                                  \\
	                       & = \var(T) + (b_{T(\theta)}(T))^2
\end{align*}
Per definizione, se $T$ è uno stimatore non deviato per $\tau(\theta)$ allora $b_{\tau(\theta)}(T)=0$ e quindi l'errore quadratico medio di $T$ è uguale alla sua varianza.


\subsection{Stimatori consistenti}
\begin{defin}[Stimatore consistente in media quadratica]
	Uno stimatore $T_n$\footnote{Più precisamente, si parla di una famiglia di stimatori $T_n$ i cui membri differiscono per dimensione del campione} è consistente in media quadratica per $\tau(\theta)$ se
	\begin{equation*}
		\lim_{n\to+\infty} \MSE_{\tau(\theta)}(T_n) = 0
	\end{equation*}
\end{defin}

\begin{defin}[Stimatore debolmente consistente]
	Uno stimatore $T_n=t(X_1,\dots,X_n)$ è debolmente consistente rispetto a una quantità ignota $\tau(\theta)$ se e solo se
	\begin{equation*}
		\forall\varepsilon>0 \quad \lim_{n\to+\infty} P(\tau(\theta)-\varepsilon \leq T_n \leq \tau(\theta)+\varepsilon) = 1
	\end{equation*}
\end{defin}

\begin{teor}
	Se uno stimatore $T$ è consistente in media quadratica per $\tau(\theta)$ allora $T$ è debolmente consistente per $\tau(\theta)$.
\end{teor}
\begin{proof}
	\begin{align*}
		P(-\varepsilon\leq T_n-\tau(\theta)\leq \varepsilon) & = P(\abs{T_n-\tau(\theta)}\leq\varepsilon)                                                                         \\
		                                                     & = P((T_n-\tau(\theta))^2\leq\varepsilon^2)                                                                         \\
		                                                     & = 1 - P((T_n-\tau(\theta))^2>\varepsilon^2)                                                                        \\
		                                                     & > 1-\frac{\ev{(T_n-\tau(\theta))^2}}{\varepsilon^2} \bc{\parbox{35mm}{disuguaglianza \ref{teor:markov} di Markov}} \\
		                                                     & = 1-\frac{\MSE_{\tau(\theta)}(T_n)}{\varepsilon^2} \to 1
	\end{align*}
	Essendo le probabilità limitate superiormente da $1$, per il teorema del confronto la probabilità iniziale tende a $1$.
\end{proof}


\subsection{Legge dei grandi numeri}
\begin{teor}[legge dei grandi numeri forte]
	\begin{equation*}
		\lim_{n\to+\infty} P\left(\bar X_n = \mu\right) = 1
	\end{equation*}
\end{teor}

\begin{teor}[legge dei grandi numeri debole]
	\begin{equation*}
		\forall\varepsilon > 0 \qquad \lim_{n\to+\infty} P\left(\abs{\bar X_n - \mu} > \varepsilon\right) = 0
	\end{equation*}
\end{teor}


\subsection{Problema dello scarto}
Un problema tipo che fa uso del teorema centrale del limite nella statistica inferenziale è il seguente.

Sia $\bar X_n$ la media campionaria dipendente da $n$ elementi e sia $\mu$ il suo valore atteso. Si vuole che lo scarto $\abs{\bar X_n - \mu}$ tra i due valori sia minore di una soglia $r$ con probabilità superiore a $1-\delta$ (con $\delta$ piccolo a piacere):
\begin{equation*}
	P(\abs{\bar X_n - \mu} \leq r) \geq 1-\delta \\
\end{equation*}

Normalizzando:
\begin{align*}
	P\left(\frac{\abs{\bar X_n - \mu}}{\sigma/\sqrt{n}} \leq \frac{r}{\sigma/\sqrt{n}}\right) & = P\left(\abs{\frac{\bar X_n - \mu}{\sigma/\sqrt{n}}} \leq \frac{r\sqrt{n}}{\sigma}\right)               \\
	                                                                                          & \approx P\left(\abs{Z} \leq \frac{r}{\sigma}\sqrt{n}\right)                                              \\
	                                                                                          & = P\left(-\frac{r}{\sigma}\sqrt{n}\leq Z\leq \frac{r}{\sigma}\sqrt{n}\right)                             \\
	                                                                                          & = \Phi\left(\frac{r}{\sigma}\sqrt{n}\right) - \Phi\left(- \frac{r}{\sigma}\sqrt{n}\right)                \\
	                                                                                          & = \Phi\left(\frac{r}{\sigma}\sqrt{n}\right) - \left(1 - \Phi\left(\frac{r}{\sigma}\sqrt{n}\right)\right)
\end{align*}

Ergo:
\begin{align*}
	2\Phi\left(\frac{r}{\sigma}\sqrt{n}\right) - 1 & \geq 1 - \delta                                                              \\
	\Phi\left(\frac{r}{\sigma}\sqrt{n}\right)      & \geq 1 - \frac{\delta}{2}                                                    \\
	\frac{r}{\sigma}\sqrt{n}                       & \geq \Phi^{-1}\left(1-\frac{\delta}{2}\right)                                \\
	n                                              & \geq \left(\frac{\sigma}{r}\Phi^{-1}\left(1-\frac{\delta}{2}\right)\right)^2
\end{align*}

Allo stesso modo si possono ricavare gli altri parametri del problema:
\begin{gather*}
	r = \frac{\sigma}{\sqrt{n}}\Phi^{-1}\left(1-\frac{\delta}{2}\right) \\
	\delta \geq 2\left(1-\Phi\left(\frac{r}{\sigma}\sqrt{n}\right)\right)
\end{gather*}
% TODO: verificare che $\sqrt{s^2}$ sia uno stimatore non deviato per $\sigma$
Mentre $\sigma$ tipicamente è stimato come $\sqrt{s^2}$ (dove $s$ è la deviazione standard campionaria).

Un'alternativa a questo tipo di stima si può fare applicando la disuguaglianza di Tchebishev:
% TODO: r=\varepsilon, coerenza a riguardo
% TODO: avere più uniformità tra i due metodi
\begin{align*}
	P(\abs{X-\mu}\geq r)                                                       & \leq \frac{\sigma^2}{r^2}     \\
	P(\abs{X-\mu} < r) = 1-P(\abs{X-\mu}\geq r)                                & \geq 1 - \frac{\sigma^2}{r^2} \\
	P(\abs{\bar X-\mu} < \varepsilon) \geq 1 - \frac{\sigma^2}{n\varepsilon^2} & \geq 1-\delta                 \\
	\frac{\sigma^2}{n\varepsilon^2}                                            & \leq \delta                   \\
\end{align*}
% TODO: spiegazione
\begin{gather*}
	n \geq \frac{\sigma^2}{\delta\varepsilon^2} \Rightarrow P(\abs{\bar X-\mu}<\varepsilon) \geq 1-\delta \\
	\delta\geq\frac{\sigma^2}{n\varepsilon^2} \\[1ex]
	\varepsilon \geq \sqrt{\frac{\sigma^2}{n\delta}}
\end{gather*}


% TODO: tutta la sezione sul processo di Poisson va rivista.
% TODO: definizione di processo stocastico
\subsection{Processo di Poisson}
Il processo di Poisson dimostra che sotto certe ipotesi il problema del numero di eventi che avvengono in un certo intervallo di tempo è descritto da un modello di Poisson.

\begin{teor}
	Sia $t$ una variabile temporale e $N(t)$ la variabile aleatoria che esprime il numero di eventi che avvengono nell'intervallo di tempo $[0,t)$. Verificate le seguenti ipotesi:
	\begin{enumerate}
		\item $N(0)=0$;
		\item istanze di $N$ sono indipendenti per intervalli disgiunti;
		      % TODO: spiegare la versione approssimata dell'ipotesi 3
		\item $\displaystyle\lim_{h\to0}\frac{P(N(h)=1)}{h}=\lambda$;
		\item $\displaystyle\lim_{h\to0}\frac{P(N(h)\geq 2)}{h}=0$.
	\end{enumerate}
	allora $N(t)$ è distribuita secondo il modello di Poisson con parametro $\lambda t$:
	\begin{equation*}
		N(t)\sim P(\lambda t)
	\end{equation*}
\end{teor}

\begin{proof}
	Volendo calcolare la massa di probabilità $P(N(t)=k)$, si sceglie $n\in\N$ e si divide l'intervallo $[0,t)$ in $n$ parti uguali $[\frac{m}{n}t,\frac{m+1}{n}t)$. Potendo scegliere $n$ grande a piacere si può scegliere in particolare $n>k$.

	L'evento $N(t)=k$, di cui si vuole calcolare la probabilità, è esprimibile come l'unione disgiunta di due eventi:
	\begin{itemize}
		\item $A$: ognuno dei $k$ eventi avviene in un intervallo diverso, ossia: ogni intervallo contiene al più un evento, per un totale di $k$ eventi;
		\item $B$: esiste almeno un intervallo che contiene due eventi, per un totale di $k$ eventi.
	\end{itemize}

	Ricordando che $n$ è grande a piacere, per l'ipotesi 4 la probabilità che uno specifico intervallo contenga due eventi tende a $0$. Inoltre, per l'ipotesi 2 la probabilità dell'evento $B$ è calcolabile come il prodotto delle singole. La probabilità complessiva è quindi il prodotto di fattori tendenti a $0$ ed è pertanto tendente a $0$ per $n\to+\infty$. Si ha quindi:
	\begin{equation*}
		P(N(t)=k)=P(A)
	\end{equation*}

	Siano $B_i$ le variabili aleatorie bernoulliane che descrivono l'esistenza o meno di un evento nell'$i$-esimo intervallo. Per l'ipotesi 3 si ha, per $n\to+\infty$:
	\begin{equation*}
		B_i \sim B\left(\lambda\frac{t}{n}\right)
	\end{equation*}
	Si noti inoltre che l'evento $A$ corrisponde a una particolare specificazione della variabile aleatoria binomiale $B'$ che descrive il numero di eventi complessivi in $n$ intervalli, ossia la somma dei $B_i$:
	\begin{equation*}
		B'\sim B\left(n, \lambda\frac{t}{n}\right)
	\end{equation*}

	Ma allora:
	\begin{gather*}
		P(N(t)=k) = P(A) = P(B'=k) \\
		N(t) = B'
	\end{gather*}

	Poiché il prodotto dei parametri della binomiale trovata è costante, ed essendo $n\to+\infty$, per quanto visto al paragrafo \ref{subsub:binompois} la distribuzione è ben descritta da un modello di Poisson di parametro $\lambda t$:
	\begin{equation*}
		N(t) \sim P(\lambda t)
	\end{equation*}
	% TODO: esponenziale per il tempo trascorso da un evento all'altro
\end{proof}
